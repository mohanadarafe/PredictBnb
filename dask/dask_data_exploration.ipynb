{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprising-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "import dask.array as da\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import csv, sys\n",
    "import multiprocessing\n",
    "from nltk import word_tokenize\n",
    "sys.path.append('../pyspark')\n",
    "from utility import *\n",
    "\n",
    "filename_train = \"../dataset/train.csv\"\n",
    "filename_test = \"../dataset/valid.csv\"\n",
    "NUMBER_OF_CPU = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-status",
   "metadata": {},
   "source": [
    "# Load data in Dask Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "canadian-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def load_data(trainFile, testFile):\n",
    "    panda_train = pd.read_csv(trainFile)\n",
    "    panda_test = pd.read_csv(testFile)\n",
    "    train_df = dd.from_pandas(panda_train, npartitions=NUMBER_OF_CPU)\n",
    "    test_df = dd.from_pandas(panda_test, npartitions=NUMBER_OF_CPU)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "explicit-zimbabwe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 59.00%\n",
      "Time on CPU: 3:18:30.120000\n",
      "Memory in use: 3.50GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 13.60%\n",
      "Time on CPU: 3:18:32.450000\n",
      "Memory in use: 3.43GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "----------> Execution Time: 0.93966 seconds\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(filename_train, filename_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-collins",
   "metadata": {},
   "source": [
    "Below, we can see that the dataframe is succesfully imported into a Dask dataframe. Now, we need to extract the information we need & build a trainig & testing Dataframe that we will use for the later stages. \n",
    "\n",
    "**We are setting the number of partitions relative to the number of CPUs available in your machine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of partitions: {NUMBER_OF_CPU}')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "threaded-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(partition):\n",
    "    title = partition.Title\n",
    "    body = partition.Body\n",
    "    return title + \" \" + body\n",
    "\n",
    "def get_quality(partition):\n",
    "    return partition.Y\n",
    "\n",
    "@metrics\n",
    "def clean_data(train, test):\n",
    "    train[\"X_trn\"] = train.map_partitions(get_question, meta=str)\n",
    "    train[\"y_trn\"] = train.map_partitions(get_quality, meta=str)\n",
    "    test[\"X_tst\"] = test.map_partitions(get_question, meta=str)\n",
    "    test[\"y_tst\"] = test.map_partitions(get_quality, meta=str)\n",
    "    new_train = train.drop(['Id', 'Title', 'Body', 'CreationDate', 'Y', 'Tags'], axis=1)\n",
    "    new_test = test.drop(['Id', 'Title', 'Body', 'CreationDate', 'Y', 'Tags'], axis=1)\n",
    "    return new_train, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "level-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 35.50%\n",
      "Time on CPU: 3:27:38.560000\n",
      "Memory in use: 3.44GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 25.80%\n",
      "Time on CPU: 3:27:38.850000\n",
      "Memory in use: 3.44GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "----------> Execution Time: 0.04751 seconds\n"
     ]
    }
   ],
   "source": [
    "training, testing = clean_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-force",
   "metadata": {},
   "source": [
    "Below, you can see our training Dask Dataframe that we created by concatenating the Title & Body and their corresponding output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abroad-envelope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               X_tst    y_tst\n",
      "0  How to get all the child records from differen...  LQ_EDIT\n",
      "1  Retrieve all except some data of the another t...  LQ_EDIT\n",
      "2  Pandas: read_html <p>I'm trying to extract US ...       HQ\n",
      "3  Reader Always gimme NULL I'm so new to C#, I w...  LQ_EDIT\n",
      "4  php rearrange array elements based on conditio...  LQ_EDIT\n",
      "\n",
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "X_tst    object\n",
      "y_tst    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(testing.head())\n",
    "print(f'\\n{type(testing)}')\n",
    "print(testing.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-joyce",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "In this part, we will preprocess our data by cleaning the text. Then, we will build a bag of word model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-detail",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "Just like in Spark, we clean our data by going through the following steps:\n",
    "\n",
    "- Lowercase questions\n",
    "- Tokenize each question\n",
    "- Remove all stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regional-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = [] \n",
    "with open(\"../dataset/stop_words.txt\", \"r\") as r:\n",
    "    STOPWORDS = r.read().split('\\n')\n",
    "\n",
    "@metrics\n",
    "def preprocess_data(training, testing):\n",
    "    if isinstance(training.head().loc[0, 'X_trn'], str):\n",
    "        training[\"X_trn\"] = training[\"X_trn\"].str.lower()\n",
    "        training[\"X_trn\"] = training[\"X_trn\"].replace(to_replace=\"(\\\\W)+\", value=' ', regex=True)\n",
    "        training['X_trn'] = training['X_trn'].apply(lambda x: [token for token in x.split(\" \")], meta=str)\n",
    "        training['X_trn'] = training['X_trn'].apply(lambda x: [token for token in x if token not in STOPWORDS], meta=str)\n",
    "        training['X_trn'] = training['X_trn'].apply(lambda x: [token for token in x if token], meta=str)\n",
    "        training['X_trn'] = training['X_trn'].apply(lambda x: \" \".join(x), meta=str)\n",
    "        \n",
    "    if isinstance(testing.head().loc[0, 'X_tst'], str):\n",
    "        testing[\"X_tst\"] = testing[\"X_tst\"].str.lower()\n",
    "        testing[\"X_tst\"] = testing[\"X_tst\"].replace(to_replace=\"(\\\\W)+\", value=' ', regex=True)\n",
    "        testing['X_tst'] = testing['X_tst'].apply(lambda x: [token for token in x.split(\" \")], meta=str)\n",
    "        testing['X_tst'] = testing['X_tst'].apply(lambda x: [token for token in x if token not in STOPWORDS], meta=str)\n",
    "        testing['X_tst'] = testing['X_tst'].apply(lambda x: [token for token in x if token], meta=str)\n",
    "        testing['X_tst'] = testing['X_tst'].apply(lambda x: \" \".join(x), meta=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "accurate-enzyme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 25.90%\n",
      "Time on CPU: 3:31:19.060000\n",
      "Memory in use: 3.43GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 0.00%\n",
      "Time on CPU: 3:31:26.760000\n",
      "Memory in use: 3.44GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "----------> Execution Time: 6.60484 seconds\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "human-elevation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_trn</th>\n",
       "      <th>y_trn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>java repeat task every random seconds p alread...</td>\n",
       "      <td>LQ_CLOSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>java optionals immutable p like understand jav...</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text overlay image darkened opacity react nati...</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ternary operator swift picky p question simple...</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hide show fab scale animation p using custom f...</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               X_trn     y_trn\n",
       "0  java repeat task every random seconds p alread...  LQ_CLOSE\n",
       "1  java optionals immutable p like understand jav...        HQ\n",
       "2  text overlay image darkened opacity react nati...        HQ\n",
       "3  ternary operator swift picky p question simple...        HQ\n",
       "4  hide show fab scale animation p using custom f...        HQ"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-playlist",
   "metadata": {},
   "source": [
    "### Create BoW model\n",
    "In this part, we want to create a bag of word model. The X will be a DataFrame where each column represents a word, each row represents a question & the number of times the word occur in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alert-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.feature_extraction.text import CountVectorizer\n",
    "from dask_ml import preprocessing\n",
    "\n",
    "def compute_chunks(X_train, y_train, X_test, y_test):\n",
    "    X_train.compute_chunk_sizes()\n",
    "    y_train.compute_chunk_sizes()\n",
    "    X_test.compute_chunk_sizes()\n",
    "    y_test.compute_chunk_sizes()\n",
    "    \n",
    "def convert_X_data(train, test):\n",
    "    X_train = train.map_blocks(lambda x: x.toarray(), dtype=int)\n",
    "    X_test = test.map_blocks(lambda x: x.toarray(), dtype=int)\n",
    "    return X_train, X_test\n",
    "\n",
    "@metrics\n",
    "def build_bow_model(training, testing):\n",
    "    vectorizer = CountVectorizer()\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    print(\"Converting to Dask Databags...\")\n",
    "    X_train_db = db.from_sequence(training['X_trn'], npartitions=NUMBER_OF_CPU)\n",
    "    X_test_db = db.from_sequence(testing['X_tst'], npartitions=NUMBER_OF_CPU)\n",
    "\n",
    "    print(\"Building BoW...\")\n",
    "    X_model = vectorizer.fit(X_train_db)\n",
    "    X_train = X_model.transform(X_train_db)\n",
    "    X_test = X_model.transform(X_test_db)\n",
    "\n",
    "    print(\"Indexing strings...\")\n",
    "    y_model = encoder.fit(training['y_trn'])\n",
    "    y_train = y_model.transform(training['y_trn'])\n",
    "    y_test = y_model.transform(testing['y_tst'])\n",
    "    \n",
    "    print(\"Computing chunks...\")\n",
    "    compute_chunks(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"Re-convert to Dask Array\")\n",
    "    Xtrain, Xtest = convert_X_data(X_train, X_test)\n",
    "        \n",
    "    return Xtrain, y_train, Xtest, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "spanish-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 20.80%\n",
      "Time on CPU: 4:09:25.560000\n",
      "Memory in use: 2.17GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "Converting to Dask Databags...\n",
      "Building BoW...\n",
      "Indexing strings...\n",
      "Computing chunks...\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 0.00%\n",
      "Time on CPU: 4:21:54.900000\n",
      "Memory in use: 2.57GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "----------> Execution Time: 703.77465 seconds\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = build_bow_model(training, testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-preservation",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Let's train our model using our training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "standard-accent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from dask_ml.naive_bayes import GaussianNB\n",
    "\n",
    "@metrics\n",
    "def train_model(x_train, y_train):\n",
    "    clf = ParallelPostFit(estimator = GaussianNB(), scoring='accuracy')\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "satisfied-renaissance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 13.30%\n",
      "Time on CPU: 4:22:28.140000\n",
      "Memory in use: 2.54GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 0.00%\n",
      "Time on CPU: 4:24:23.780000\n",
      "Memory in use: 2.57GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.64GiB\n",
      "\n",
      "----------> Execution Time: 93.40049 seconds\n"
     ]
    }
   ],
   "source": [
    "clf = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "jewish-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-taiwan",
   "metadata": {},
   "source": [
    "# Run entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "realistic-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############LOADING DATA###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 6.60%\n",
      "Time on CPU: 0:02:59.490000\n",
      "Memory in use: 5.09GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 5.20%\n",
      "Time on CPU: 0:03:00.710000\n",
      "Memory in use: 5.03GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "----------> Execution Time: 0.82981 seconds\n",
      "###############CLEANING DATA###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 1.60%\n",
      "Time on CPU: 0:03:00.730000\n",
      "Memory in use: 5.03GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 4.90%\n",
      "Time on CPU: 0:03:00.820000\n",
      "Memory in use: 5.03GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "----------> Execution Time: 0.02635 seconds\n",
      "###############PREPROCESSING DATA###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 8.30%\n",
      "Time on CPU: 0:03:00.870000\n",
      "Memory in use: 5.03GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 10.00%\n",
      "Time on CPU: 0:03:01.050000\n",
      "Memory in use: 5.02GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "----------> Execution Time: 0.15228 seconds\n",
      "###############BUILDING BOW###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 23.00%\n",
      "Time on CPU: 0:03:01.190000\n",
      "Memory in use: 5.02GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "Converting to Dask Databags...\n",
      "Building BoW...\n",
      "Indexing strings...\n",
      "Computing chunks...\n",
      "Re-convert to Dask Array\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 0.00%\n",
      "Time on CPU: 0:07:43.940000\n",
      "Memory in use: 4.58GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "----------> Execution Time: 274.78575 seconds\n",
      "###############TRAINING DATA###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 0.00%\n",
      "Time on CPU: 0:07:43.950000\n",
      "Memory in use: 4.58GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 0.00%\n",
      "Time on CPU: 0:08:22.090000\n",
      "Memory in use: 4.57GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "----------> Execution Time: 37.64520 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"###############LOADING DATA###############\")\n",
    "train_df, test_df = load_data(filename_train, filename_test)\n",
    "\n",
    "print(\"###############CLEANING DATA###############\")\n",
    "training, testing = clean_data(train_df, test_df)\n",
    "\n",
    "print(\"###############PREPROCESSING DATA###############\")\n",
    "preprocess_data(training, testing)\n",
    "\n",
    "print(\"###############BUILDING BOW###############\")\n",
    "X_train, y_train, X_test, y_test = build_bow_model(training, testing)\n",
    "\n",
    "print(\"###############TRAINING DATA###############\")\n",
    "clf = train_model(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
