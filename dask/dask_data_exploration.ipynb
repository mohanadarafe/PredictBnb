{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fatty-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "import dask.array as da\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import csv, sys\n",
    "import multiprocessing\n",
    "from nltk import word_tokenize\n",
    "sys.path.append('../pyspark')\n",
    "from utility import *\n",
    "\n",
    "filename_train = \"../dataset/train.csv\"\n",
    "filename_test = \"../dataset/valid.csv\"\n",
    "NUMBER_OF_CPU = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-worker",
   "metadata": {},
   "source": [
    "# Load data in Dask Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "convertible-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def load_data(trainFile, testFile):\n",
    "    panda_train = pd.read_csv(trainFile)\n",
    "    panda_test = pd.read_csv(testFile)\n",
    "    train_df = dd.from_pandas(panda_train, npartitions=NUMBER_OF_CPU)\n",
    "    test_df = dd.from_pandas(panda_test, npartitions=NUMBER_OF_CPU)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "handy-clinton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 59.00%\n",
      "Time on CPU: 3:18:30.120000\n",
      "Memory in use: 3.50GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 13.60%\n",
      "Time on CPU: 3:18:32.450000\n",
      "Memory in use: 3.43GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "----------> Execution Time: 0.93966 seconds\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(filename_train, filename_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-principle",
   "metadata": {},
   "source": [
    "Below, we can see that the dataframe is succesfully imported into a Dask dataframe. Now, we need to extract the information we need & build a trainig & testing Dataframe that we will use for the later stages. \n",
    "\n",
    "**We are setting the number of partitions relative to the number of CPUs available in your machine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of partitions: {NUMBER_OF_CPU}')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "respiratory-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question(partition):\n",
    "    title = partition.Title\n",
    "    body = partition.Body\n",
    "    return title + \" \" + body\n",
    "\n",
    "def get_quality(partition):\n",
    "    return partition.Y\n",
    "\n",
    "@metrics\n",
    "def clean_data(train, test):\n",
    "    train[\"X_trn\"] = train.map_partitions(get_question, meta=str)\n",
    "    train[\"y_trn\"] = train.map_partitions(get_quality, meta=str)\n",
    "    test[\"X_tst\"] = test.map_partitions(get_question, meta=str)\n",
    "    test[\"y_tst\"] = test.map_partitions(get_quality, meta=str)\n",
    "    new_train = train.drop(['Id', 'Title', 'Body', 'CreationDate', 'Y', 'Tags'], axis=1)\n",
    "    new_test = test.drop(['Id', 'Title', 'Body', 'CreationDate', 'Y', 'Tags'], axis=1)\n",
    "    return new_train, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "strong-making",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 35.50%\n",
      "Time on CPU: 3:27:38.560000\n",
      "Memory in use: 3.44GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 25.80%\n",
      "Time on CPU: 3:27:38.850000\n",
      "Memory in use: 3.44GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "----------> Execution Time: 0.04751 seconds\n"
     ]
    }
   ],
   "source": [
    "training, testing = clean_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-farmer",
   "metadata": {},
   "source": [
    "Below, you can see our training Dask Dataframe that we created by concatenating the Title & Body and their corresponding output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "mechanical-colony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               X_tst    y_tst\n",
      "0  How to get all the child records from differen...  LQ_EDIT\n",
      "1  Retrieve all except some data of the another t...  LQ_EDIT\n",
      "2  Pandas: read_html <p>I'm trying to extract US ...       HQ\n",
      "3  Reader Always gimme NULL I'm so new to C#, I w...  LQ_EDIT\n",
      "4  php rearrange array elements based on conditio...  LQ_EDIT\n",
      "\n",
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "X_tst    object\n",
      "y_tst    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(testing.head())\n",
    "print(f'\\n{type(testing)}')\n",
    "print(testing.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-means",
   "metadata": {},
   "source": [
    "# Preprocess Data\n",
    "In this part, we will preprocess our data by cleaning the text. Then, we will build a bag of word model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-screening",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "Just like in Spark, we clean our data by going through the following steps:\n",
    "\n",
    "- Lowercase questions\n",
    "- Tokenize each question\n",
    "- Remove all stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "latin-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = [] \n",
    "with open(\"../dataset/stop_words.txt\", \"r\") as r:\n",
    "    STOPWORDS = r.read().split('\\n')\n",
    "\n",
    "@metrics\n",
    "def preprocess_data(training, testing):\n",
    "    if isinstance(training.head().loc[0, 'X_trn'], str):\n",
    "        training[\"X_trn\"] = training[\"X_trn\"].str.lower()\n",
    "        training[\"X_trn\"] = training[\"X_trn\"].replace(to_replace=\"(\\\\W)+\", value=' ', regex=True)\n",
    "        training['X_trn'] = training['X_trn'].apply(lambda x: [token for token in x.split(\" \")], meta=str)\n",
    "        training['X_trn'] = training['X_trn'].apply(lambda x: [token for token in x if token not in STOPWORDS], meta=str)\n",
    "        training['X_trn'] = training['X_trn'].apply(lambda x: [token for token in x if token], meta=str)\n",
    "        training['X_trn'] = training['X_trn'].apply(lambda x: \" \".join(x), meta=str)\n",
    "        \n",
    "    if isinstance(testing.head().loc[0, 'X_tst'], str):\n",
    "        testing[\"X_tst\"] = testing[\"X_tst\"].str.lower()\n",
    "        testing[\"X_tst\"] = testing[\"X_tst\"].replace(to_replace=\"(\\\\W)+\", value=' ', regex=True)\n",
    "        testing['X_tst'] = testing['X_tst'].apply(lambda x: [token for token in x.split(\" \")], meta=str)\n",
    "        testing['X_tst'] = testing['X_tst'].apply(lambda x: [token for token in x if token not in STOPWORDS], meta=str)\n",
    "        testing['X_tst'] = testing['X_tst'].apply(lambda x: [token for token in x if token], meta=str)\n",
    "        testing['X_tst'] = testing['X_tst'].apply(lambda x: \" \".join(x), meta=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "swedish-television",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 25.90%\n",
      "Time on CPU: 3:31:19.060000\n",
      "Memory in use: 3.43GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 0.00%\n",
      "Time on CPU: 3:31:26.760000\n",
      "Memory in use: 3.44GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.66GiB\n",
      "\n",
      "----------> Execution Time: 6.60484 seconds\n"
     ]
    }
   ],
   "source": [
    "preprocess_data(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "brown-footwear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_trn</th>\n",
       "      <th>y_trn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>java repeat task every random seconds p alread...</td>\n",
       "      <td>LQ_CLOSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>java optionals immutable p like understand jav...</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text overlay image darkened opacity react nati...</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ternary operator swift picky p question simple...</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hide show fab scale animation p using custom f...</td>\n",
       "      <td>HQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               X_trn     y_trn\n",
       "0  java repeat task every random seconds p alread...  LQ_CLOSE\n",
       "1  java optionals immutable p like understand jav...        HQ\n",
       "2  text overlay image darkened opacity react nati...        HQ\n",
       "3  ternary operator swift picky p question simple...        HQ\n",
       "4  hide show fab scale animation p using custom f...        HQ"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-universe",
   "metadata": {},
   "source": [
    "### Create BoW model\n",
    "In this part, we want to create a bag of word model. The X will be a DataFrame where each column represents a word, each row represents a question & the number of times the word occur in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "modified-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.feature_extraction.text import CountVectorizer\n",
    "from dask_ml import preprocessing\n",
    "\n",
    "def compute_chunks(X_train, y_train, X_test, y_test):\n",
    "    X_train.compute_chunk_sizes()\n",
    "    y_train.compute_chunk_sizes()\n",
    "    X_test.compute_chunk_sizes()\n",
    "    y_test.compute_chunk_sizes()\n",
    "    \n",
    "def convert_X_data(train, test):\n",
    "    X_train = train.map_blocks(lambda x: x.toarray(), dtype=int)\n",
    "    X_test = test.map_blocks(lambda x: x.toarray(), dtype=int)\n",
    "    return X_train, X_test\n",
    "\n",
    "@metrics\n",
    "def build_bow_model(training, testing):\n",
    "    vectorizer = CountVectorizer()\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    \n",
    "    print(\"Converting to Dask Databags...\")\n",
    "    X_train_db = db.from_sequence(training['X_trn'], npartitions=NUMBER_OF_CPU)\n",
    "    X_test_db = db.from_sequence(testing['X_tst'], npartitions=NUMBER_OF_CPU)\n",
    "\n",
    "    print(\"Building BoW...\")\n",
    "    X_model = vectorizer.fit(X_train_db)\n",
    "    X_train = X_model.transform(X_train_db)\n",
    "    X_test = X_model.transform(X_test_db)\n",
    "\n",
    "    print(\"Indexing strings...\")\n",
    "    y_model = encoder.fit(training['y_trn'])\n",
    "    y_train = y_model.transform(training['y_trn'])\n",
    "    y_test = y_model.transform(testing['y_tst'])\n",
    "    \n",
    "    print(\"Computing chunks...\")\n",
    "    compute_chunks(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"Re-convert to Dask Array\")\n",
    "    Xtrain, Xtest = convert_X_data(X_train, X_test)\n",
    "        \n",
    "    return Xtrain, y_train, Xtest, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-tobacco",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "described-college",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 20.80%\n",
      "Time on CPU: 4:09:25.560000\n",
      "Memory in use: 2.17GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "Converting to Dask Databags...\n",
      "Building BoW...\n",
      "Indexing strings...\n",
      "Computing chunks...\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 0.00%\n",
      "Time on CPU: 4:21:54.900000\n",
      "Memory in use: 2.57GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.65GiB\n",
      "\n",
      "----------> Execution Time: 703.77465 seconds\n"
     ]
    }
   ],
   "source": [
    "vectorizer, encoder = init_bow(training, testing)\n",
    "X_model, y_model, X_train_db, X_test_db = build_bow(vectorizer, encoder)\n",
    "X_train, y_train, X_test, y_test = transform_bow(X_model, y_model, X_train_db, X_test_db, training, testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-merchant",
   "metadata": {},
   "source": [
    "# Train model\n",
    "Let's train our model using our training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "solved-emperor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from dask_ml.naive_bayes import GaussianNB\n",
    "\n",
    "@metrics\n",
    "def train_model(x_train, y_train):\n",
    "    clf = ParallelPostFit(estimator = GaussianNB(), scoring='accuracy')\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conventional-treasure",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-47795e4ac778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "clf = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bigger-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-phrase",
   "metadata": {},
   "source": [
    "# Run entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "julian-stability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############LOADING DATA###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 43.10%\n",
      "Time on CPU: 0:47:22.450000\n",
      "Memory in use: 3.29GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.00GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 47.50%\n",
      "Time on CPU: 0:47:24.630000\n",
      "Memory in use: 3.28GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.00GiB\n",
      "\n",
      "----------> Execution Time: 1.00071 seconds\n",
      "###############CLEANING DATA###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 48.30%\n",
      "Time on CPU: 0:47:24.930000\n",
      "Memory in use: 3.29GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.00GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 14.80%\n",
      "Time on CPU: 0:47:25.130000\n",
      "Memory in use: 3.29GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.00GiB\n",
      "\n",
      "----------> Execution Time: 0.03078 seconds\n",
      "###############PREPROCESSING DATA###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 28.80%\n",
      "Time on CPU: 0:47:25.300000\n",
      "Memory in use: 3.29GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.00GiB\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 13.80%\n",
      "Time on CPU: 0:47:25.560000\n",
      "Memory in use: 3.29GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.00GiB\n",
      "\n",
      "----------> Execution Time: 0.07562 seconds\n",
      "###############BUILDING BOW###############\n",
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "CPU in use: 39.70%\n",
      "Time on CPU: 0:47:25.800000\n",
      "Memory in use: 3.29GiB\n",
      "Disk in use: 6.10%\n",
      "Disk free: 795.00GiB\n",
      "Converting to Dask Databags...\n",
      "Building BoW...\n",
      "Indexing strings...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ebe7cf9821fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"###############BUILDING BOW###############\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_bow_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"###############TRAINING DATA###############\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Concordia/SOEN471/StackOverflow-Quality-Predictor/pyspark/utility.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0minfo_before_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n --------- AFTER CALL TO FUNCTION ---------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-b17b00624f8a>\u001b[0m in \u001b[0;36mbuild_bow_model\u001b[0;34m(training, testing)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Indexing strings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0my_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_trn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_trn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_tst'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stack/lib/python3.8/site-packages/dask_ml/preprocessing/label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode_dask_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0m_is_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stack/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stack/lib/python3.8/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stack/lib/python3.8/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mpools\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     results = get_async(\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stack/lib/python3.8/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;31m# Main loop, wait on tasks to finish, insert new ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"waiting\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                     \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stack/lib/python3.8/site-packages/dask/local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stack/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/stack/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"###############LOADING DATA###############\")\n",
    "train_df, test_df = load_data(filename_train, filename_test)\n",
    "\n",
    "print(\"###############CLEANING DATA###############\")\n",
    "training, testing = clean_data(train_df, test_df)\n",
    "\n",
    "print(\"###############PREPROCESSING DATA###############\")\n",
    "preprocess_data(training, testing)\n",
    "\n",
    "print(\"###############BUILDING BOW###############\")\n",
    "X_train, y_train, Xtest, y_test = build_bow_model(training, testing)\n",
    "\n",
    "print(\"###############TRAINING DATA###############\")\n",
    "clf = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "underlying-framework",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
