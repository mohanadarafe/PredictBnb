{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "armed-deputy",
   "metadata": {},
   "source": [
    "# PySpark Implementation\n",
    "In this notebook, we will implement train a NB model using the training set & preprocess the data with our heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-condition",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "nominated-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, sys\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opened-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spark session.\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impossible-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-convenience",
   "metadata": {},
   "source": [
    "### Converting training data into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "incomplete-deviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "|      Id|               Title|                Body|                Tags|       CreationDate|       Y|\n",
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "|34552656|Java: Repeat Task...|<p>I'm already fa...|      <java><repeat>|2016-01-01 00:21:59|LQ_CLOSE|\n",
      "|34553034|Why are Java Opti...|<p>I'd like to un...|    <java><optional>|2016-01-01 02:03:20|      HQ|\n",
      "|34553174|Text Overlay Imag...|<p>I am attemptin...|<javascript><imag...|2016-01-01 02:48:24|      HQ|\n",
      "|34553318|Why ternary opera...|<p>The question i...|<swift><operators...|2016-01-01 03:30:17|      HQ|\n",
      "|34553755|hide/show fab wit...|<p>I'm using cust...|<android><materia...|2016-01-01 05:21:48|      HQ|\n",
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename_train = \"../dataset/train.csv\"\n",
    "filename_test = \"../dataset/valid.csv\"\n",
    "\n",
    "train_rdd = spark.read.csv(filename_train, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "test_rdd = spark.read.csv(filename_test, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "train_rdd.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-elephant",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "In this part, we will preprocess our data with three different heuristics. The first using tokenization, second using stemming, third removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-anatomy",
   "metadata": {},
   "source": [
    "### Build training & testing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "careful-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = train_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"]) \\\n",
    "    .limit(1000) # change to collect()\n",
    "\n",
    "testing = test_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"]) \\\n",
    "    .limit(1000) # change to collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-knife",
   "metadata": {},
   "source": [
    "### Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "structured-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEURISTIC 1 - Tokenize the words\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Question\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# HEURISTIC 2 - Remove the stopwords\n",
    "add_stopwords = [\"the\", \"a\", \"be\", \"of\", \"and\", \"to\", \"why\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# HEURISTIC 3 - Stem words\n",
    "# Use this library (TODO) - https://github.com/master/spark-stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aware-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build B.o.W model \n",
    "countVectors_h1 = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "indexed_features_h1 = StringIndexer(inputCol = \"Output\", outputCol = \"label\")\n",
    "\n",
    "countVectors_h2 = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "indexed_features_h2 = StringIndexer(inputCol = \"Output\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-chance",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "asian-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with Heurisitc 1\n",
    "pipeline_h1 = Pipeline(stages=[regexTokenizer, countVectors_h1, indexed_features_h1])\n",
    "\n",
    "# Pipeline with Heurisitc 2\n",
    "pipeline_h2 = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors_h2, indexed_features_h2])\n",
    "\n",
    "# Pipeline with Heurisitc 3\n",
    "# pipeline_h3 = Pipeline(stages=[regexTokenizer, stopwordsRemover, stemRemover, countVectors, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "optimum-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|Java: Repeat Task...|LQ_CLOSE|[java, repeat, ta...|(12668,[0,1,2,3,4...|  1.0|\n",
      "|Why are Java Opti...|      HQ|[why, are, java, ...|(12668,[0,1,4,9,1...|  0.0|\n",
      "|Text Overlay Imag...|      HQ|[text, overlay, i...|(12668,[0,1,2,3,4...|  0.0|\n",
      "|Why ternary opera...|      HQ|[why, ternary, op...|(12668,[0,1,2,3,4...|  0.0|\n",
      "|hide/show fab wit...|      HQ|[hide, show, fab,...|(12668,[0,1,4,5,9...|  0.0|\n",
      "|Accessing pointer...|LQ_CLOSE|[accessing, point...|(12668,[0,1,2,3,4...|  1.0|\n",
      "|How To Disable 2n...| LQ_EDIT|[how, to, disable...|(12668,[1,4,11,14...|  2.0|\n",
      "|Resizing containe...| LQ_EDIT|[resizing, contai...|(12668,[1,2,3,4,5...|  2.0|\n",
      "|Changing Theme in...|      HQ|[changing, theme,...|(12668,[0,1,2,3,4...|  0.0|\n",
      "|TextBox Value Dis...| LQ_EDIT|[textbox, value, ...|(12668,[1,6,9,11,...|  2.0|\n",
      "|MongoDB Failing t...|      HQ|[mongodb, failing...|(12668,[0,1,3,4,5...|  0.0|\n",
      "|What's the best w...|LQ_CLOSE|[what, s, the, be...|(12668,[0,1,2,3,4...|  1.0|\n",
      "|ios/objective-c/x...| LQ_EDIT|[ios, objective, ...|(12668,[1,2,4,5,6...|  2.0|\n",
      "|output FILE ,is t...| LQ_EDIT|[output, file, is...|(12668,[1,2,3,4,5...|  2.0|\n",
      "|Pod install displ...|      HQ|[pod, install, di...|(12668,[0,2,3,4,5...|  0.0|\n",
      "|Haskell Stack Ghc...|      HQ|[haskell, stack, ...|(12668,[0,1,2,3,4...|  0.0|\n",
      "|Why does the reve...|      HQ|[why, does, the, ...|(12668,[0,1,2,3,4...|  0.0|\n",
      "|eb deploy does no...|      HQ|[eb, deploy, does...|(12668,[0,1,2,3,4...|  0.0|\n",
      "|How to create a f...| LQ_EDIT|[how, to, create,...|(12668,[1,2,3,4,5...|  2.0|\n",
      "|bluebird.js vs bl...|      HQ|[bluebird, js, vs...|(12668,[0,1,2,4,5...|  0.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_h1_fit = pipeline_h1.fit(training)\n",
    "data_h1_train = pipeline_h1_fit.transform(training)\n",
    "data_h1_train.show()\n",
    "\n",
    "# pipeline_h2_fit = pipeline_h2.fit(training)\n",
    "# data_h2 = pipeline_h2_fit.transform(training)\n",
    "# data_h2.show()\n",
    "\n",
    "# pipeline_h3_fit = pipeline_h3.fit(training)\n",
    "# data_h3 = pipeline_h3_fit.transform(training)\n",
    "# data_h3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-prospect",
   "metadata": {},
   "source": [
    "## Model training\n",
    "In this part, we will train various pipelines through a NB classifier. \n",
    "- Hypertune the parameters\n",
    "- Train the model\n",
    "- Get the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-plaintiff",
   "metadata": {},
   "source": [
    "### Hypertuning parameters\n",
    "Generally speaking, NB models don't require a lot of hypertuning, but, it remains good practice nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nuclear-potential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82722876568036"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split training data into train/validation set.\n",
    "train_h1, validate_h1 = data_h1_train.randomSplit([0.7, 0.3], seed = 1234)\n",
    "\n",
    "# Create grid to find best smoothing\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build()\n",
    "cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "\n",
    "# Cross-validate all smoothing values\n",
    "cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(train_h1)\n",
    "\n",
    "# Make predictions on validation set\n",
    "cvPredictions = cvModel.transform(validate_h1)\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-modification",
   "metadata": {},
   "source": [
    "### Training best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-explorer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
