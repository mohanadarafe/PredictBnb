{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bottom-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, sys\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from time import time\n",
    "from datetime import timedelta,datetime\n",
    "from sys import argv\n",
    "import psutil as psu\n",
    "from utility import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continuous-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spark session.\n",
    "@metrics\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "white-malta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- BEFORE CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "Memory in use: 5.14GiB\n",
      "Disk in use: 71.70%\n",
      "Disk free: 4.11GiB\n",
      "Time on CPU: 4:04:10.400000\n",
      "CPU in use: 14.50%\n",
      "\n",
      "\n",
      " --------- AFTER CALL TO FUNCTION ---------\n",
      "\n",
      "\n",
      " ========== SYSTEM INFO ===========\n",
      "\n",
      "Memory in use: 5.12GiB\n",
      "Disk in use: 71.70%\n",
      "Disk free: 4.11GiB\n",
      "Time on CPU: 4:04:23.410000\n",
      "CPU in use: 36.50%\n",
      "\n",
      "----------> Execution Time: 3.94852 seconds\n"
     ]
    }
   ],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-watch",
   "metadata": {},
   "source": [
    "# Load data from file into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "american-england",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "|      Id|               Title|                Body|                Tags|       CreationDate|       Y|\n",
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "|34552656|Java: Repeat Task...|<p>I'm already fa...|      <java><repeat>|2016-01-01 00:21:59|LQ_CLOSE|\n",
      "|34553034|Why are Java Opti...|<p>I'd like to un...|    <java><optional>|2016-01-01 02:03:20|      HQ|\n",
      "|34553174|Text Overlay Imag...|<p>I am attemptin...|<javascript><imag...|2016-01-01 02:48:24|      HQ|\n",
      "|34553318|Why ternary opera...|<p>The question i...|<swift><operators...|2016-01-01 03:30:17|      HQ|\n",
      "|34553755|hide/show fab wit...|<p>I'm using cust...|<android><materia...|2016-01-01 05:21:48|      HQ|\n",
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename_train = \"../dataset/train.csv\"\n",
    "filename_test = \"../dataset/valid.csv\"\n",
    "\n",
    "train_rdd = spark.read.csv(filename_train, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "test_rdd = spark.read.csv(filename_test, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "train_rdd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "suspected-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = train_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"])# change to collect()\n",
    "\n",
    "testing = test_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"]) # change to collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-windsor",
   "metadata": {},
   "source": [
    "### Associated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "understood-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def load_train_test_rdd(filename_train, filename_test):\n",
    "    train_rdd = spark.read.csv(filename_train, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "    test_rdd = spark.read.csv(filename_test, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "    return train_rdd, test_rdd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "automated-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def transform_rdd_to_df(train_rdd,test_rdd):\n",
    "    training = train_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"])\n",
    "\n",
    "    testing = test_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"])\n",
    "    \n",
    "    return training, testing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "secret-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = \"../dataset/train.csv\"\n",
    "filename_test = \"../dataset/valid.csv\"\n",
    "train_rdd, test_rdd = load_train_test_rdd(filename_train, filename_test)\n",
    "train_df, test_df = transform_rdd_to_df(train_rdd,test_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-camping",
   "metadata": {},
   "source": [
    "# Prepare pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "classified-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_word_remover(input_col_name, stopwords):\n",
    "    return StopWordsRemover(inputCol=input_col_name, outputCol=\"filtered\").setStopWords(stopwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "automotive-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def get_heuristics(stop_word_file):\n",
    "    # HEURISTIC 1 - Tokenize the words\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Question\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "    \n",
    "    # HEURISTIC 2 - Remove the stopwords\n",
    "    stop_words = []\n",
    "    with open(stop_word_file, \"r\") as text_file:\n",
    "        stop_words = text_file.read().split('\\n')\n",
    "    \n",
    "    stopwordsRemover = get_stop_word_remover(\"words\", stop_words)\n",
    "    \n",
    "    return regexTokenizer, stopwordsRemover\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bottom-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_file = \"../dataset/stop_words.txt\"\n",
    "regexTokenizer, stopwordsRemover = get_heuristics(stop_word_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "diverse-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def get_bag_of_word_model(features_col_name, label_col_name):\n",
    "    countVectors = CountVectorizer(inputCol=features_col_name, outputCol=\"features\")\n",
    "    indexed_features = StringIndexer(inputCol = label_col_name, outputCol = \"label\")\n",
    "    return countVectors, indexed_features\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "unauthorized-planet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 7.7%\n",
      "Uptime: 19:21:47.528509\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.625556945800781GiB\n",
      "Dsik free: 5.49658203125GiB\n",
      "Time on CPU: 7:36:48.240000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:21:47.660760\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.624839782714844GiB\n",
      "Dsik free: 5.49658203125GiB\n",
      "Time on CPU: 7:36:48.240000\n",
      "Execution Time: 0.03131103515625\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:21:47.761858\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.624351501464844GiB\n",
      "Dsik free: 5.49658203125GiB\n",
      "Time on CPU: 7:36:48.240000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:21:47.871834\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.623928070068359GiB\n",
      "Dsik free: 5.49658203125GiB\n",
      "Time on CPU: 7:36:48.240000\n",
      "Execution Time: 0.009263992309570312\n"
     ]
    }
   ],
   "source": [
    "countVectors_h1, indexed_features_h1 = get_bag_of_word_model(\"words\", \"Output\")\n",
    "countVectors_h2, indexed_features_h2 = get_bag_of_word_model(\"filtered\", \"Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "serial-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def get_pipeline(*args):\n",
    "    return Pipeline(stages=[*args])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "victorian-cradle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 6.3%\n",
      "Uptime: 19:21:52.424083\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.653079986572266GiB\n",
      "Dsik free: 5.496364593505859GiB\n",
      "Time on CPU: 7:36:51.890000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 4.9%\n",
      "Uptime: 19:21:52.527166\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.653156280517578GiB\n",
      "Dsik free: 5.496364593505859GiB\n",
      "Time on CPU: 7:36:51.940000\n",
      "Execution Time: 0.0004260540008544922\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 4.8%\n",
      "Uptime: 19:21:52.628059\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.653175354003906GiB\n",
      "Dsik free: 5.496364593505859GiB\n",
      "Time on CPU: 7:36:51.980000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:21:52.734210\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.653194427490234GiB\n",
      "Dsik free: 5.496364593505859GiB\n",
      "Time on CPU: 7:36:51.980000\n",
      "Execution Time: 0.0003941059112548828\n"
     ]
    }
   ],
   "source": [
    "pipeline_h1 = get_pipeline(regexTokenizer, stopwordsRemover, countVectors_h2, indexed_features_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-pontiac",
   "metadata": {},
   "source": [
    "# Process data through Pipeline train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "limiting-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def get_pipeline_model(pipeline, data):\n",
    "    \"\"\" We should use the same pipeline model on training and testing \"\"\"\n",
    "    return pipeline.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "electoral-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def transform_data_through_pipeline(model, data):\n",
    "    data_transformed = model.transform(data)\n",
    "    return data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "incorporated-immigration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16.91487216949463\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|Java: Repeat Task...|LQ_CLOSE|[java, repeat, ta...|(201488,[0,1,2,3,...|  1.0|\n",
      "|Why are Java Opti...|      HQ|[why, are, java, ...|(201488,[0,1,4,7,...|  0.0|\n",
      "|Text Overlay Imag...|      HQ|[text, overlay, i...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Why ternary opera...|      HQ|[why, ternary, op...|(201488,[0,1,2,3,...|  0.0|\n",
      "|hide/show fab wit...|      HQ|[hide, show, fab,...|(201488,[0,1,4,5,...|  0.0|\n",
      "|Accessing pointer...|LQ_CLOSE|[accessing, point...|(201488,[0,1,2,3,...|  1.0|\n",
      "|How To Disable 2n...| LQ_EDIT|[how, to, disable...|(201488,[1,4,8,15...|  2.0|\n",
      "|Resizing containe...| LQ_EDIT|[resizing, contai...|(201488,[1,2,3,4,...|  2.0|\n",
      "|Changing Theme in...|      HQ|[changing, theme,...|(201488,[0,1,2,3,...|  0.0|\n",
      "|TextBox Value Dis...| LQ_EDIT|[textbox, value, ...|(201488,[1,6,7,8,...|  2.0|\n",
      "|MongoDB Failing t...|      HQ|[mongodb, failing...|(201488,[0,1,3,4,...|  0.0|\n",
      "|What's the best w...|LQ_CLOSE|[what, s, the, be...|(201488,[0,1,2,3,...|  1.0|\n",
      "|ios/objective-c/x...| LQ_EDIT|[ios, objective, ...|(201488,[1,2,4,5,...|  2.0|\n",
      "|output FILE ,is t...| LQ_EDIT|[output, file, is...|(201488,[1,2,3,4,...|  2.0|\n",
      "|Pod install displ...|      HQ|[pod, install, di...|(201488,[0,2,3,4,...|  0.0|\n",
      "|Haskell Stack Ghc...|      HQ|[haskell, stack, ...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Why does the reve...|      HQ|[why, does, the, ...|(201488,[0,1,2,3,...|  0.0|\n",
      "|eb deploy does no...|      HQ|[eb, deploy, does...|(201488,[0,1,2,3,...|  0.0|\n",
      "|How to create a f...| LQ_EDIT|[how, to, create,...|(201488,[1,2,3,4,...|  2.0|\n",
      "|bluebird.js vs bl...|      HQ|[bluebird, js, vs...|(201488,[0,1,2,4,...|  0.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_pipeline = get_pipeline_model(pipeline_h1, training)\n",
    "data_h1_train = transform_data_through_pipeline(model_pipeline, train_df)\n",
    "data_h1_test = transform_data_through_pipeline(model_pipeline, test_df)\n",
    "data_h1_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-might",
   "metadata": {},
   "source": [
    "**Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "religious-ordinance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1293400': 1, 'mypass': 5, 'connected': 433, 'few': 902, 'input': 9663, 'online': 599, '389999': 1, 'travel': 74, '836400': 1, 'those': 1212, 'still': 2194, 'thread1': 26, 'hope': 513, 'recognize': 120, 'parentheses': 89, 'arguments': 696, 'persist': 74, '2bxhsys2c47eyjfhpmroalpxz5suigeubqu7hjuvfvwpoa0xri3iljvhq5qgbwtwpe1x0': 2, 'pabu': 1, 'some': 8532}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary frequency without using the countVectorizer helper column that we generated\n",
    "counts = data_h1_train.select(f.explode('words').alias('col')).groupBy('col').count().take(20)\n",
    "print({row['col']: row['count'] for row in counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "collect-project",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7b4ae0958939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# using the countVectorizer helper column that we generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_h1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# using the countVectorizer helper column that we generated\n",
    "# https://stackoverflow.com/questions/50255356/pyspark-countvectorizer-and-word-frequency-in-a-corpus\n",
    "# counts = data_h1_train.select('words').take(20)\n",
    "# print(dict(zip(vocabulary, counts[0]['words'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "authentic-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|How to get all th...| LQ_EDIT|[how, to, get, al...|(86590,[1,2,4,6,9...|  2.0|\n",
      "|Retrieve all exce...| LQ_EDIT|[retrieve, all, e...|(86590,[1,2,7,9,1...|  2.0|\n",
      "|Pandas: read_html...|      HQ|[pandas, read_htm...|(86590,[0,1,2,3,4...|  0.0|\n",
      "|Reader Always gim...| LQ_EDIT|[reader, always, ...|(86590,[1,2,4,5,7...|  2.0|\n",
      "|php rearrange arr...| LQ_EDIT|[php, rearrange, ...|(86590,[1,2,4,5,6...|  2.0|\n",
      "|How do I make a c...|LQ_CLOSE|[how, do, i, make...|(86590,[0,1,2,3,4...|  1.0|\n",
      "|how can i create ...| LQ_EDIT|[how, can, i, cre...|(86590,[1,5,6,9,1...|  2.0|\n",
      "|Re-exporting ES6 ...|      HQ|[re, exporting, e...|(86590,[0,1,2,3,4...|  0.0|\n",
      "|Fetch API with Co...|      HQ|[fetch, api, with...|(86590,[0,1,2,4,5...|  0.0|\n",
      "|Print list conten...|LQ_CLOSE|[print, list, con...|(86590,[0,1,2,3,4...|  1.0|\n",
      "|c# - List all pri...| LQ_EDIT|[c, list, all, pr...|(86590,[1,2,3,4,5...|  2.0|\n",
      "|Angular2 exceptio...|      HQ|[angular2, except...|(86590,[0,3,11,21...|  0.0|\n",
      "|Form Validation p...|LQ_CLOSE|[form, validation...|(86590,[0,1,2,4,5...|  1.0|\n",
      "|Most Pythonic way...|      HQ|[most, pythonic, ...|(86590,[0,1,2,3,4...|  0.0|\n",
      "|Gulp error intern...|      HQ|[gulp, error, int...|(86590,[0,1,2,4,9...|  0.0|\n",
      "|Filter Name with ...| LQ_EDIT|[filter, name, wi...|(86590,[1,2,3,4,6...|  2.0|\n",
      "|Django ImageField...|      HQ|[django, imagefie...|(86590,[0,1,2,3,4...|  0.0|\n",
      "|Compiling SASS in...|LQ_CLOSE|[compiling, sass,...|(86590,[0,1,4,5,6...|  1.0|\n",
      "|to get or set the...| LQ_EDIT|[to, get, or, set...|(86590,[1,2,3,4,5...|  2.0|\n",
      "|i am new to pythn...| LQ_EDIT|[i, am, new, to, ...|(86590,[1,3,4,7,9...|  2.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_h1_test = process_data_in_pipeline(pipeline_h1, testing)\n",
    "data_h1_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "valued-nursing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|How to get all th...| LQ_EDIT|[how, to, get, al...|(201488,[1,2,4,6,...|  2.0|\n",
      "|Retrieve all exce...| LQ_EDIT|[retrieve, all, e...|(201488,[1,2,7,8,...|  2.0|\n",
      "|Pandas: read_html...|      HQ|[pandas, read_htm...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Reader Always gim...| LQ_EDIT|[reader, always, ...|(201488,[1,2,4,5,...|  2.0|\n",
      "|php rearrange arr...| LQ_EDIT|[php, rearrange, ...|(201488,[1,2,4,5,...|  2.0|\n",
      "|How do I make a c...|LQ_CLOSE|[how, do, i, make...|(201488,[0,1,2,3,...|  1.0|\n",
      "|how can i create ...| LQ_EDIT|[how, can, i, cre...|(201488,[1,5,6,8,...|  2.0|\n",
      "|Re-exporting ES6 ...|      HQ|[re, exporting, e...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Fetch API with Co...|      HQ|[fetch, api, with...|(201488,[0,1,2,4,...|  0.0|\n",
      "|Print list conten...|LQ_CLOSE|[print, list, con...|(201488,[0,1,2,3,...|  1.0|\n",
      "|c# - List all pri...| LQ_EDIT|[c, list, all, pr...|(201488,[1,2,3,4,...|  2.0|\n",
      "|Angular2 exceptio...|      HQ|[angular2, except...|(201488,[0,3,11,2...|  0.0|\n",
      "|Form Validation p...|LQ_CLOSE|[form, validation...|(201488,[0,1,2,4,...|  1.0|\n",
      "|Most Pythonic way...|      HQ|[most, pythonic, ...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Gulp error intern...|      HQ|[gulp, error, int...|(201488,[0,1,2,4,...|  0.0|\n",
      "|Filter Name with ...| LQ_EDIT|[filter, name, wi...|(201488,[1,2,3,4,...|  2.0|\n",
      "|Django ImageField...|      HQ|[django, imagefie...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Compiling SASS in...|LQ_CLOSE|[compiling, sass,...|(201488,[0,1,4,5,...|  1.0|\n",
      "|to get or set the...| LQ_EDIT|[to, get, or, set...|(201488,[1,2,3,4,...|  2.0|\n",
      "|i am new to pythn...| LQ_EDIT|[i, am, new, to, ...|(201488,[1,3,4,7,...|  2.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_h1_test  = model_pipeline.transform(testing)\n",
    "data_h1_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-linux",
   "metadata": {},
   "source": [
    "# Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "disciplinary-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def split_dataset(data, distribution):\n",
    "    return data.randomSplit([distribution, 1-distribution], seed = 1234)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "developmental-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_h1, validate_h1 = split_dataset(data_h1_train, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-reminder",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning | is this doing anything? we are not using any data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "material-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def get_best_smoothing_values(target_col, prediction_col):\n",
    "    # Create grid to find best smoothing\n",
    "    nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "    paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build()\n",
    "    cvEvaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=prediction_col)\n",
    "\n",
    "    # Cross-validate all smoothing values\n",
    "    cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "    return cv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "driven-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = get_best_smoothing_values(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-juice",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "oriented-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def train_naive_bayes_model(cv, data):\n",
    "    model = cv.fit(data)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spare-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = train_naive_bayes_model(cv,train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-classroom",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cross-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def predict(model, data):\n",
    "    predictions = model.transform(data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "metropolitan-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(nb_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "welsh-colon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cvPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "widespread-headline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  2.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvPredictions.select(\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "numerical-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvPredictions_test = cvModel.transform(data_h1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "artificial-bobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  2.0|       2.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  2.0|       2.0|\n",
      "|  2.0|       2.0|\n",
      "|  1.0|       1.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  2.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  2.0|       2.0|\n",
      "|  2.0|       2.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvPredictions_test.select(\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-rotation",
   "metadata": {},
   "source": [
    "# Evaluate Performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "searching-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "@metrics\n",
    "def evaluate_model(target_col, prediction_col, predictionAndTarget):\n",
    "    evaluatorMulti = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=prediction_col)\n",
    "    # Get metrics\n",
    "    acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "    f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "    weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "    weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "    print(\"\\n******** Metrics **********\")\n",
    "    print (\"Model Accuracy: {:.3f}%\".format(acc*100))\n",
    "    print (\"Model f1-score: {:.3f}%\".format(f1*100))\n",
    "    print (\"Model weightedPrecision: {:.3f}%\".format(weightedPrecision*100))\n",
    "    print (\"Model weightedRecall: {:.3f}%\".format(weightedRecall*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "exclusive-chocolate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.7653008490217793\n",
      "Model f1-score:  0.7675841714238029\n",
      "Model weightedPrecision:  0.7901628512971111\n",
      "Model weightedRecall:  0.7653008490217792\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(\"label\", \"prediction\", cvPredictions.select(\"label\",\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dried-italic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.7633333333333333\n",
      "Model f1-score:  0.7649182102284617\n",
      "Model weightedPrecision:  0.7866094786891012\n",
      "Model weightedRecall:  0.7633333333333333\n"
     ]
    }
   ],
   "source": [
    "# testing dataset\n",
    "evaluate_model(\"label\", \"prediction\", cvPredictions_test.select(\"label\",\"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-australia",
   "metadata": {},
   "source": [
    "# Creating one big iteration for all the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "silent-david",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_spark():\n",
    "    print(\"##########################################\")\n",
    "    print(\"############# Start Spark ################\")\n",
    "    print(\"##########################################\")\n",
    "    global spark\n",
    "    spark = init_spark()\n",
    "    filename_train = \"../dataset/train.csv\"\n",
    "    filename_test = \"../dataset/valid.csv\"\n",
    "    stop_word_file = \"../dataset/stop_words.txt\"\n",
    "    print(\"\\n#########################################\")\n",
    "    print(\"####### Load dataset in spark rdd #######\")\n",
    "    print(\"###########################################\")\n",
    "    train_rdd, test_rdd = load_train_test_rdd(filename_train, filename_test)\n",
    "    print(\"\\n#########################################\")\n",
    "    print(\"########## Transform rdd to df ############\")\n",
    "    print(\"##########################################\")\n",
    "    train_df, test_df = transform_rdd_to_df(train_rdd,test_rdd)\n",
    "    print(\"\\n##########################################\")\n",
    "    print(\"########## Create Heuristics #############\")\n",
    "    print(\"##########################################\")\n",
    "    regexTokenizer, stopwordsRemover = get_heuristics(stop_word_file)\n",
    "    countVectors_h1, indexed_features_h1 = get_bag_of_word_model(\"words\", \"Output\")\n",
    "    countVectors_h2, indexed_features_h2 = get_bag_of_word_model(\"filtered\", \"Output\")\n",
    "    print(\"\\n##########################################\")\n",
    "    print(\"############ Construct pipeline ############\")\n",
    "    print(\"############################################\")\n",
    "    pipeline = get_pipeline(regexTokenizer, stopwordsRemover, countVectors_h2, indexed_features_h2)\n",
    "    print(\"\\n##########################################\")\n",
    "    print(\"########### Train pipeline model ###########\")\n",
    "    print(\"############################################\")\n",
    "    model_pipeline = get_pipeline_model(pipeline, train_df)\n",
    "    print(\"\\n#############################################\")\n",
    "    print(\"####### Transform train data through pipeline #####\")\n",
    "    print(\"################################################\")\n",
    "    train = transform_data_through_pipeline(model_pipeline, train_df)\n",
    "    print(\"\\n##################################################\")\n",
    "    print(\"####### Transform test data through pipeline #######\")\n",
    "    print(\"##################################################\")\n",
    "    test = transform_data_through_pipeline(model_pipeline, test_df)\n",
    "    print(\"\\n###################################################\")\n",
    "    print(\"####### Train naive base classifier model #######\")\n",
    "    print(\"####################################################\")\n",
    "    cv = get_best_smoothing_values(\"label\", \"prediction\")\n",
    "    nb_model = train_naive_bayes_model(cv,train)\n",
    "    print(\"\\n##############################################################\")\n",
    "    print(\"### Predict test data using naive base classifier model ######\")\n",
    "    print(\"################################################################\")\n",
    "    predictions = predict(nb_model, test)\n",
    "    print(\"####################################\")\n",
    "    print(\"####### Evaluate predictions #######\")\n",
    "    print(\"#####################################\")\n",
    "    evaluate_model(\"label\", \"prediction\",predictions.select(\"label\",\"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-contemporary",
   "metadata": {},
   "source": [
    "# Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flush-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator\n",
    "def metrics(fun):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('--------- BEFORE CALL TO FUNCTION ---------')\n",
    "        fun_name = fun.__name__\n",
    "        info_before_call = system_info()\n",
    "        start = time()\n",
    "        rv = fun(*args, **kwargs)\n",
    "        duration = time() - start\n",
    "        print('\\n\\n --------- AFTER CALL TO FUNCTION ---------')\n",
    "        info_after_call = system_info()\n",
    "        system_dict = {fun_name: [info_before_call, info_after_call, duration]}\n",
    "\n",
    "        print(\"\\n----------> Execution Time: {:.5f} seconds\".format(duration))\n",
    "\n",
    "        with open(\"system_info.json\", \"r+\") as file:\n",
    "            data = json.load(file)\n",
    "            data.update(system_dict)\n",
    "            file.seek(0)\n",
    "            json.dump(data, file, default=str)\n",
    "        return rv\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recreational-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_info():\n",
    "    info = {\n",
    "        \"CPU in use\": \"{:.2f}%\".format(psu.cpu_percent(interval=.1)),\n",
    "        \"Time on CPU\": timedelta(seconds=psu.cpu_times().system + psu.cpu_times().user),\n",
    "        \"Memory in use\": \"{:.2f}GiB\".format(psu.virtual_memory().available / (1024 ** 3)),\n",
    "        \"Disk in use\": \"{:.2f}%\".format(psu.disk_usage('/').percent),\n",
    "        \"Disk free\": \"{:.2f}GiB\".format(psu.disk_usage('/').free / (1024 ** 3)),\n",
    "    }\n",
    "\n",
    "    print(\"\\n\\n ========== SYSTEM INFO ===========\\n\\n\" + \"\\n\".join(\n",
    "        [\"{}: {}\".format(key, value) for key, value in info.items()]))\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "naked-holly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 15.0%\n",
      "Uptime: 19:15:26.625819\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.625919342041016MiB\n",
      "Dsik free: 5.499961853027344MiB\n",
      "Time on CPU: 7:32:57.350000\n"
     ]
    }
   ],
   "source": [
    "system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "olympic-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:15:19.796094\n"
     ]
    }
   ],
   "source": [
    "print(timedelta(seconds=time()-psu.boot_time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wicked-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds=time()-psu.boot_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "freelance-multiple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "signed-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_info():\n",
    "    processes_info = []\n",
    "    for process in psu.process_iter(attrs=(\n",
    "            'name', 'cmdline', 'pid', 'create_time', 'cpu_percent', 'cpu_times', 'num_threads', 'memory_percent')):\n",
    "\n",
    "        if process.info[\"name\"] is not None and (\"python3.5\" in process.info[\"name\"] or \"java\" in process.info[\"name\"]):\n",
    "            mem = process.info['memory_percent']\n",
    "            info = {\n",
    "                \"PID\": process.info[\"pid\"],\n",
    "                \"Process name\": process.info[\"name\"],\n",
    "                \"Current time\": datetime.fromtimestamp(time()).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"Create time\": datetime.fromtimestamp(process.create_time()).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"Uptime\": timedelta(seconds=time() - process.info[\"create_time\"]),\n",
    "                \"CPU in use\": \"{:.2f}%\".format(process.info['cpu_percent']),\n",
    "                \"Time on CPU\": timedelta(seconds=process.info[\"cpu_times\"].system + process.info[\"cpu_times\"].user),\n",
    "                \"Nb of threads\": process.info[\"num_threads\"],\n",
    "                \"Memory in use\": \"{:.2f}%\".format(mem),\n",
    "                \"Memory_usage\": \"{:.2f} GiB\".format(psu.virtual_memory().total * (mem / 100) / (1024 ** 3)),\n",
    "            }\n",
    "            processes_info.append(info)\n",
    "            print(\"\\n\\n ********* PROCESS INFO *********\\n\\n\" + \"\\n\".join(\n",
    "                [\"{}: {}\".format(key, value) for key, value in info.items()]))\n",
    "\n",
    "    process_dict = {\"processes\": processes_info}\n",
    "\n",
    "    with open('processes_info.json', 'w') as fp:\n",
    "        json.dump(process_dict, fp, default=str)\n",
    "\n",
    "    return processes_info           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "executive-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " PROCESS INFO\n",
      "\n",
      "Uptime: 12:13:20.380172\n",
      "Create time: 2021-04-10 23:46:55\n",
      "Nb of threads: 4\n",
      "Memory_usage: 0.03 GiB\n",
      "Memory in use: 0.16%\n",
      "CPU in use: 0.20%\n",
      "PID: 5065\n",
      "Time on CPU: 0:00:06.343331\n",
      "\n",
      "\n",
      " PROCESS INFO\n",
      "\n",
      "Uptime: 12:13:05.514102\n",
      "Create time: 2021-04-10 23:47:10\n",
      "Nb of threads: 10\n",
      "Memory_usage: 0.03 GiB\n",
      "Memory in use: 0.18%\n",
      "CPU in use: 0.20%\n",
      "PID: 5078\n",
      "Time on CPU: 0:00:03.614826\n",
      "\n",
      "\n",
      " PROCESS INFO\n",
      "\n",
      "Uptime: 12:12:23.593419\n",
      "Create time: 2021-04-10 23:47:52\n",
      "Nb of threads: 1\n",
      "Memory_usage: 0.00 GiB\n",
      "Memory in use: 0.00%\n",
      "CPU in use: 0.00%\n",
      "PID: 5095\n",
      "Time on CPU: 0:00:00.970705\n"
     ]
    }
   ],
   "source": [
    "process_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-extraction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
