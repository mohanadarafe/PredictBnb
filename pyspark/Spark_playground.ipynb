{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "third-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, sys\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "from sys import argv\n",
    "import psutil as psu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "stopped-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spark session.\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "auburn-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-dietary",
   "metadata": {},
   "source": [
    "# Load data from file into spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moral-distributor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "|      Id|               Title|                Body|                Tags|       CreationDate|       Y|\n",
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "|34552656|Java: Repeat Task...|<p>I'm already fa...|      <java><repeat>|2016-01-01 00:21:59|LQ_CLOSE|\n",
      "|34553034|Why are Java Opti...|<p>I'd like to un...|    <java><optional>|2016-01-01 02:03:20|      HQ|\n",
      "|34553174|Text Overlay Imag...|<p>I am attemptin...|<javascript><imag...|2016-01-01 02:48:24|      HQ|\n",
      "|34553318|Why ternary opera...|<p>The question i...|<swift><operators...|2016-01-01 03:30:17|      HQ|\n",
      "|34553755|hide/show fab wit...|<p>I'm using cust...|<android><materia...|2016-01-01 05:21:48|      HQ|\n",
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename_train = \"../dataset/train.csv\"\n",
    "filename_test = \"../dataset/valid.csv\"\n",
    "\n",
    "train_rdd = spark.read.csv(filename_train, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "test_rdd = spark.read.csv(filename_test, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "train_rdd.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "liable-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = train_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"])# change to collect()\n",
    "\n",
    "testing = test_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"]) # change to collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-behalf",
   "metadata": {},
   "source": [
    "# Prepare pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "promotional-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_stop_word_remover(input_col_name, stopwords):\n",
    "    return StopWordsRemover(inputCol=input_col_name, outputCol=\"filtered\").setStopWords(stopwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "upset-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 6.6%\n",
      "Uptime: 19:18:48.718848\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.674312591552734GiB\n",
      "Dsik free: 5.499122619628906GiB\n",
      "Time on CPU: 7:35:15.750000\n",
      "AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 5.1%\n",
      "Uptime: 19:18:48.898850\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.673244476318359GiB\n",
      "Dsik free: 5.499122619628906GiB\n",
      "Time on CPU: 7:35:16\n",
      "Execution Time: 0.07918906211853027\n"
     ]
    }
   ],
   "source": [
    "# HEURISTIC 1 - Tokenize the words\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Question\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# HEURISTIC 2 - Remove the stopwords\n",
    "# Reading stopwords file and storing each word in a list by specifying the return to line delimiter\n",
    "stop_words = []\n",
    "text_file = open(\"../dataset/stop_words.txt\", \"r\")\n",
    "stop_words = text_file.read().split('\\n')\n",
    "add_stopwords = [\"the\", \"a\", \"be\", \"of\", \"and\", \"to\", \"why\"] \n",
    "stopwordsRemover = get_stop_word_remover(\"words\", stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "deluxe-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_bag_of_word_model(features_col_name, label_col_name):\n",
    "    countVectors = CountVectorizer(inputCol=features_col_name, outputCol=\"features\")\n",
    "    indexed_features = StringIndexer(inputCol = label_col_name, outputCol = \"label\")\n",
    "    return countVectors, indexed_features\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "distinguished-pittsburgh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 7.7%\n",
      "Uptime: 19:21:47.528509\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.625556945800781GiB\n",
      "Dsik free: 5.49658203125GiB\n",
      "Time on CPU: 7:36:48.240000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:21:47.660760\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.624839782714844GiB\n",
      "Dsik free: 5.49658203125GiB\n",
      "Time on CPU: 7:36:48.240000\n",
      "Execution Time: 0.03131103515625\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:21:47.761858\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.624351501464844GiB\n",
      "Dsik free: 5.49658203125GiB\n",
      "Time on CPU: 7:36:48.240000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:21:47.871834\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.623928070068359GiB\n",
      "Dsik free: 5.49658203125GiB\n",
      "Time on CPU: 7:36:48.240000\n",
      "Execution Time: 0.009263992309570312\n"
     ]
    }
   ],
   "source": [
    "countVectors_h1, indexed_features_h1 = get_bag_of_word_model(\"words\", \"Output\")\n",
    "countVectors_h2, indexed_features_h2 = get_bag_of_word_model(\"filtered\", \"Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "closing-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_pipeline(*args):\n",
    "    return Pipeline(stages=[*args])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "unexpected-friday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 6.3%\n",
      "Uptime: 19:21:52.424083\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.653079986572266GiB\n",
      "Dsik free: 5.496364593505859GiB\n",
      "Time on CPU: 7:36:51.890000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 4.9%\n",
      "Uptime: 19:21:52.527166\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.653156280517578GiB\n",
      "Dsik free: 5.496364593505859GiB\n",
      "Time on CPU: 7:36:51.940000\n",
      "Execution Time: 0.0004260540008544922\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 4.8%\n",
      "Uptime: 19:21:52.628059\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.653175354003906GiB\n",
      "Dsik free: 5.496364593505859GiB\n",
      "Time on CPU: 7:36:51.980000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:21:52.734210\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.653194427490234GiB\n",
      "Dsik free: 5.496364593505859GiB\n",
      "Time on CPU: 7:36:51.980000\n",
      "Execution Time: 0.0003941059112548828\n"
     ]
    }
   ],
   "source": [
    "pipeline_h1 = get_pipeline(regexTokenizer, countVectors_h1, indexed_features_h1)\n",
    "pipeline_h2 = get_pipeline(regexTokenizer, stopwordsRemover, countVectors_h2, indexed_features_h2)\n",
    "# pipeline_h3 = get_pipeline(regexTokenizer, stopwordsRemover, countVectors_h2, indexed_features_h2,stemmer_udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-acceptance",
   "metadata": {},
   "source": [
    "# Process data through Pipeline train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cloudy-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_pipeline_model(pipeline, data):\n",
    "    \"\"\" We should use the same pipeline model on training and testing \"\"\"\n",
    "    return pipeline.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-circular",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "exterior-alignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16.91487216949463\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|Java: Repeat Task...|LQ_CLOSE|[java, repeat, ta...|(201488,[0,1,2,3,...|  1.0|\n",
      "|Why are Java Opti...|      HQ|[why, are, java, ...|(201488,[0,1,4,7,...|  0.0|\n",
      "|Text Overlay Imag...|      HQ|[text, overlay, i...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Why ternary opera...|      HQ|[why, ternary, op...|(201488,[0,1,2,3,...|  0.0|\n",
      "|hide/show fab wit...|      HQ|[hide, show, fab,...|(201488,[0,1,4,5,...|  0.0|\n",
      "|Accessing pointer...|LQ_CLOSE|[accessing, point...|(201488,[0,1,2,3,...|  1.0|\n",
      "|How To Disable 2n...| LQ_EDIT|[how, to, disable...|(201488,[1,4,8,15...|  2.0|\n",
      "|Resizing containe...| LQ_EDIT|[resizing, contai...|(201488,[1,2,3,4,...|  2.0|\n",
      "|Changing Theme in...|      HQ|[changing, theme,...|(201488,[0,1,2,3,...|  0.0|\n",
      "|TextBox Value Dis...| LQ_EDIT|[textbox, value, ...|(201488,[1,6,7,8,...|  2.0|\n",
      "|MongoDB Failing t...|      HQ|[mongodb, failing...|(201488,[0,1,3,4,...|  0.0|\n",
      "|What's the best w...|LQ_CLOSE|[what, s, the, be...|(201488,[0,1,2,3,...|  1.0|\n",
      "|ios/objective-c/x...| LQ_EDIT|[ios, objective, ...|(201488,[1,2,4,5,...|  2.0|\n",
      "|output FILE ,is t...| LQ_EDIT|[output, file, is...|(201488,[1,2,3,4,...|  2.0|\n",
      "|Pod install displ...|      HQ|[pod, install, di...|(201488,[0,2,3,4,...|  0.0|\n",
      "|Haskell Stack Ghc...|      HQ|[haskell, stack, ...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Why does the reve...|      HQ|[why, does, the, ...|(201488,[0,1,2,3,...|  0.0|\n",
      "|eb deploy does no...|      HQ|[eb, deploy, does...|(201488,[0,1,2,3,...|  0.0|\n",
      "|How to create a f...| LQ_EDIT|[how, to, create,...|(201488,[1,2,3,4,...|  2.0|\n",
      "|bluebird.js vs bl...|      HQ|[bluebird, js, vs...|(201488,[0,1,2,4,...|  0.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model_pipeline = pipeline_h1.fit(training)\n",
    "model_pipeline = get_pipeline_model(pipeline_h1, training)\n",
    "data_h1_train  = model_pipeline.transform(training) #what does the transform do?\n",
    "data_h1_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-tennis",
   "metadata": {},
   "source": [
    "**Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "direct-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1293400': 1, 'mypass': 5, 'connected': 433, 'few': 902, 'input': 9663, 'online': 599, '389999': 1, 'travel': 74, '836400': 1, 'those': 1212, 'still': 2194, 'thread1': 26, 'hope': 513, 'recognize': 120, 'parentheses': 89, 'arguments': 696, 'persist': 74, '2bxhsys2c47eyjfhpmroalpxz5suigeubqu7hjuvfvwpoa0xri3iljvhq5qgbwtwpe1x0': 2, 'pabu': 1, 'some': 8532}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary frequency without using the countVectorizer helper column that we generated\n",
    "counts = data_h1_train.select(f.explode('words').alias('col')).groupBy('col').count().take(20)\n",
    "print({row['col']: row['count'] for row in counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "institutional-treasure",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-7b4ae0958939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# using the countVectorizer helper column that we generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_h1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vectors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# using the countVectorizer helper column that we generated\n",
    "# https://stackoverflow.com/questions/50255356/pyspark-countvectorizer-and-word-frequency-in-a-corpus\n",
    "counts = data_h1_train.select('words').take(20)\n",
    "print(dict(zip(vocabulary, counts[0]['words'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "painted-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|How to get all th...| LQ_EDIT|[how, to, get, al...|(86590,[1,2,4,6,9...|  2.0|\n",
      "|Retrieve all exce...| LQ_EDIT|[retrieve, all, e...|(86590,[1,2,7,9,1...|  2.0|\n",
      "|Pandas: read_html...|      HQ|[pandas, read_htm...|(86590,[0,1,2,3,4...|  0.0|\n",
      "|Reader Always gim...| LQ_EDIT|[reader, always, ...|(86590,[1,2,4,5,7...|  2.0|\n",
      "|php rearrange arr...| LQ_EDIT|[php, rearrange, ...|(86590,[1,2,4,5,6...|  2.0|\n",
      "|How do I make a c...|LQ_CLOSE|[how, do, i, make...|(86590,[0,1,2,3,4...|  1.0|\n",
      "|how can i create ...| LQ_EDIT|[how, can, i, cre...|(86590,[1,5,6,9,1...|  2.0|\n",
      "|Re-exporting ES6 ...|      HQ|[re, exporting, e...|(86590,[0,1,2,3,4...|  0.0|\n",
      "|Fetch API with Co...|      HQ|[fetch, api, with...|(86590,[0,1,2,4,5...|  0.0|\n",
      "|Print list conten...|LQ_CLOSE|[print, list, con...|(86590,[0,1,2,3,4...|  1.0|\n",
      "|c# - List all pri...| LQ_EDIT|[c, list, all, pr...|(86590,[1,2,3,4,5...|  2.0|\n",
      "|Angular2 exceptio...|      HQ|[angular2, except...|(86590,[0,3,11,21...|  0.0|\n",
      "|Form Validation p...|LQ_CLOSE|[form, validation...|(86590,[0,1,2,4,5...|  1.0|\n",
      "|Most Pythonic way...|      HQ|[most, pythonic, ...|(86590,[0,1,2,3,4...|  0.0|\n",
      "|Gulp error intern...|      HQ|[gulp, error, int...|(86590,[0,1,2,4,9...|  0.0|\n",
      "|Filter Name with ...| LQ_EDIT|[filter, name, wi...|(86590,[1,2,3,4,6...|  2.0|\n",
      "|Django ImageField...|      HQ|[django, imagefie...|(86590,[0,1,2,3,4...|  0.0|\n",
      "|Compiling SASS in...|LQ_CLOSE|[compiling, sass,...|(86590,[0,1,4,5,6...|  1.0|\n",
      "|to get or set the...| LQ_EDIT|[to, get, or, set...|(86590,[1,2,3,4,5...|  2.0|\n",
      "|i am new to pythn...| LQ_EDIT|[i, am, new, to, ...|(86590,[1,3,4,7,9...|  2.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_h1_test = process_data_in_pipeline(pipeline_h1, testing)\n",
    "data_h1_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "assured-drill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|How to get all th...| LQ_EDIT|[how, to, get, al...|(201488,[1,2,4,6,...|  2.0|\n",
      "|Retrieve all exce...| LQ_EDIT|[retrieve, all, e...|(201488,[1,2,7,8,...|  2.0|\n",
      "|Pandas: read_html...|      HQ|[pandas, read_htm...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Reader Always gim...| LQ_EDIT|[reader, always, ...|(201488,[1,2,4,5,...|  2.0|\n",
      "|php rearrange arr...| LQ_EDIT|[php, rearrange, ...|(201488,[1,2,4,5,...|  2.0|\n",
      "|How do I make a c...|LQ_CLOSE|[how, do, i, make...|(201488,[0,1,2,3,...|  1.0|\n",
      "|how can i create ...| LQ_EDIT|[how, can, i, cre...|(201488,[1,5,6,8,...|  2.0|\n",
      "|Re-exporting ES6 ...|      HQ|[re, exporting, e...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Fetch API with Co...|      HQ|[fetch, api, with...|(201488,[0,1,2,4,...|  0.0|\n",
      "|Print list conten...|LQ_CLOSE|[print, list, con...|(201488,[0,1,2,3,...|  1.0|\n",
      "|c# - List all pri...| LQ_EDIT|[c, list, all, pr...|(201488,[1,2,3,4,...|  2.0|\n",
      "|Angular2 exceptio...|      HQ|[angular2, except...|(201488,[0,3,11,2...|  0.0|\n",
      "|Form Validation p...|LQ_CLOSE|[form, validation...|(201488,[0,1,2,4,...|  1.0|\n",
      "|Most Pythonic way...|      HQ|[most, pythonic, ...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Gulp error intern...|      HQ|[gulp, error, int...|(201488,[0,1,2,4,...|  0.0|\n",
      "|Filter Name with ...| LQ_EDIT|[filter, name, wi...|(201488,[1,2,3,4,...|  2.0|\n",
      "|Django ImageField...|      HQ|[django, imagefie...|(201488,[0,1,2,3,...|  0.0|\n",
      "|Compiling SASS in...|LQ_CLOSE|[compiling, sass,...|(201488,[0,1,4,5,...|  1.0|\n",
      "|to get or set the...| LQ_EDIT|[to, get, or, set...|(201488,[1,2,3,4,...|  2.0|\n",
      "|i am new to pythn...| LQ_EDIT|[i, am, new, to, ...|(201488,[1,3,4,7,...|  2.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_h1_test  = model_pipeline.transform(testing)\n",
    "data_h1_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-vertex",
   "metadata": {},
   "source": [
    "# Split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "controlled-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def split_dataset(data, distribution):\n",
    "    return data.randomSplit([distribution, 1-distribution], seed = 1234)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coral-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_h1, validate_h1 = split_dataset(data_h1_train, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "advance-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1,test2 = split_dataset(data_h1_test, 0.7) # just for testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-robin",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning | is this doing anything? we are not using any data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "palestinian-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_best_smoothing_values(target_col, prediction_col):\n",
    "    # Create grid to find best smoothing\n",
    "    nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "    paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build()\n",
    "#     cvEvaluator = BinaryClassificationEvaluator(rawPredictionCol=prediction_col)\n",
    "    cvEvaluator = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=prediction_col)\n",
    "\n",
    "    # Cross-validate all smoothing values\n",
    "    cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "    return cv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "statistical-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = get_best_smoothing_values(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-cooper",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "silent-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(train_h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-variation",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "rocky-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation set\n",
    "cvPredictions = cvModel.transform(validate_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "social-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cvPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "radio-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  2.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvPredictions.select(\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "descending-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvPredictions_test = cvModel.transform(data_h1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "promotional-arbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  2.0|       2.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  2.0|       2.0|\n",
      "|  2.0|       2.0|\n",
      "|  1.0|       1.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  2.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  2.0|       2.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  2.0|       2.0|\n",
      "|  2.0|       2.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvPredictions_test.select(\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-suicide",
   "metadata": {},
   "source": [
    "# Evaluate Performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "enormous-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def evaluate_model(target_col, prediction_col, predictionAndTarget):\n",
    "#     evaluator = BinaryClassificationEvaluator(rawPredictionCol=prediction_col)\n",
    "    evaluatorMulti = MulticlassClassificationEvaluator(labelCol=target_col, predictionCol=prediction_col)\n",
    "#     accuracy = evaluatorMulti.evaluate(predictions)\n",
    "    # Get metrics\n",
    "    acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "    f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "    weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "    weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "#     auc = evaluator.evaluate(predictionAndTarget)\n",
    "    print (\"Model Accuracy: \", acc)\n",
    "    print (\"Model f1-score: \", f1)\n",
    "    print (\"Model weightedPrecision: \", weightedPrecision)\n",
    "    print (\"Model weightedRecall: \", weightedRecall)\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "explicit-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.7653008490217793\n",
      "Model f1-score:  0.7675841714238029\n",
      "Model weightedPrecision:  0.7901628512971111\n",
      "Model weightedRecall:  0.7653008490217792\n"
     ]
    }
   ],
   "source": [
    "# Validation Dataset\n",
    "evaluate_model(\"label\", \"prediction\", cvPredictions.select(\"label\",\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "departmental-sender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.7633333333333333\n",
      "Model f1-score:  0.7649182102284617\n",
      "Model weightedPrecision:  0.7866094786891012\n",
      "Model weightedRecall:  0.7633333333333333\n"
     ]
    }
   ],
   "source": [
    "# testing dataset\n",
    "evaluate_model(\"label\", \"prediction\", cvPredictions_test.select(\"label\",\"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-sodium",
   "metadata": {},
   "source": [
    "# Creating one big iteration for all the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "spread-remainder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######Pipeline 1#######\n",
      "**Creating pipeline model**\n",
      "Time: 16.767117977142334\n",
      "**Transforming training data**\n",
      "**Transforming testing data**\n",
      "**Hyperparameter tuning**\n",
      "Time: 0.03666400909423828\n",
      "**Training the model**\n",
      "**predicting on testing dataset**\n",
      "**Measuring performance**\n",
      "Model Accuracy:  0.7734666666666666\n",
      "Model f1-score:  0.7755420672257562\n",
      "Model weightedPrecision:  0.7947480066182155\n",
      "Model weightedRecall:  0.7734666666666666\n",
      "Time: 28.150495052337646\n",
      "#######Pipeline 2#######\n",
      "**Creating pipeline model**\n",
      "Time: 17.788862943649292\n",
      "**Transforming training data**\n",
      "**Transforming testing data**\n",
      "**Hyperparameter tuning**\n",
      "Time: 0.011992931365966797\n",
      "**Training the model**\n",
      "**predicting on testing dataset**\n",
      "**Measuring performance**\n",
      "Model Accuracy:  0.7816\n",
      "Model f1-score:  0.7840113017879425\n",
      "Model weightedPrecision:  0.7998494840023584\n",
      "Model weightedRecall:  0.7816\n",
      "Time: 33.55508613586426\n",
      "CPU times: user 1.29 s, sys: 238 ms, total: 1.53 s\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for pipeline, pipe_name in zip([pipeline_h1, pipeline_h2],[\"Pipeline 1\",\"Pipeline 2\"]):\n",
    "    print(\"#######\"+ pipe_name + \"#######\")\n",
    "    print(\"**\"+ \"Creating pipeline model\" + \"**\") \n",
    "    model_pipeline = get_pipeline_model(pipeline, training)\n",
    "    print(\"**\"+ \"Transforming training data\" + \"**\") \n",
    "    data_train  = model_pipeline.transform(training)\n",
    "    print(\"**\"+ \"Transforming testing data\" + \"**\") \n",
    "    data_test  = model_pipeline.transform(testing)\n",
    "    print(\"**\"+ \"Hyperparameter tuning\" + \"**\") \n",
    "    cv = get_best_smoothing_values(\"label\", \"prediction\")\n",
    "    print(\"**\"+ \"Training the model\" + \"**\") \n",
    "    cvModel = cv.fit(data_train)\n",
    "    print(\"**\"+ \"predicting on testing dataset\" + \"**\") \n",
    "    cvPredictions = cvModel.transform(data_test)\n",
    "    print(\"**\"+ \"Measuring performance\" + \"**\")\n",
    "    evaluate_model(\"label\", \"prediction\", cvPredictions.select(\"label\",\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "turned-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######Pipeline 1#######\n",
      "**Creating pipeline model**\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 6.3%\n",
      "Uptime: 19:22:16.616642\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.592823028564453GiB\n",
      "Dsik free: 5.492912292480469GiB\n",
      "Time on CPU: 7:37:10.150000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 7.3%\n",
      "Uptime: 19:22:35.311528\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.654502868652344GiB\n",
      "Dsik free: 5.490562438964844GiB\n",
      "Time on CPU: 7:37:53.890000\n",
      "Execution Time: 18.59303617477417\n",
      "**Transforming training data**\n",
      "**Transforming testing data**\n",
      "**Hyperparameter tuning**\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 4.9%\n",
      "Uptime: 19:22:35.705788\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.679847717285156GiB\n",
      "Dsik free: 5.490562438964844GiB\n",
      "Time on CPU: 7:37:54.660000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:22:35.849007\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.686481475830078GiB\n",
      "Dsik free: 5.490562438964844GiB\n",
      "Time on CPU: 7:37:54.660000\n",
      "Execution Time: 0.0417780876159668\n",
      "**Training the model**\n",
      "**predicting on testing dataset**\n",
      "**Measuring performance**\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 2.5%\n",
      "Uptime: 19:24:03.851043\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.5320587158203125GiB\n",
      "Dsik free: 5.49517822265625GiB\n",
      "Time on CPU: 7:41:32.970000\n",
      "Model Accuracy:  0.7726\n",
      "Model f1-score:  0.7746909975733439\n",
      "Model weightedPrecision:  0.7939332260574594\n",
      "Model weightedRecall:  0.7726\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 10.6%\n",
      "Uptime: 19:24:29.957896\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.675502777099609GiB\n",
      "Dsik free: 5.496807098388672GiB\n",
      "Time on CPU: 7:42:31.300000\n",
      "Execution Time: 26.001079082489014\n",
      "#######Pipeline 2 with new stop words#######\n",
      "**Creating pipeline model**\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 7.2%\n",
      "Uptime: 19:24:30.062591\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.675529479980469GiB\n",
      "Dsik free: 5.496807098388672GiB\n",
      "Time on CPU: 7:42:31.360000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 6.3%\n",
      "Uptime: 19:24:46.666341\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.721271514892578GiB\n",
      "Dsik free: 5.496635437011719GiB\n",
      "Time on CPU: 7:43:04.180000\n",
      "Execution Time: 16.500602960586548\n",
      "**Transforming training data**\n",
      "**Transforming testing data**\n",
      "**Hyperparameter tuning**\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 6.3%\n",
      "Uptime: 19:24:47.095612\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.7135009765625GiB\n",
      "Dsik free: 5.496635437011719GiB\n",
      "Time on CPU: 7:43:05.100000\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 0.0%\n",
      "Uptime: 19:24:47.204354\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.713527679443359GiB\n",
      "Dsik free: 5.496635437011719GiB\n",
      "Time on CPU: 7:43:05.100000\n",
      "Execution Time: 0.007964134216308594\n",
      "**Training the model**\n",
      "**predicting on testing dataset**\n",
      "**Measuring performance**\n",
      "BEFORE CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 4.9%\n",
      "Uptime: 19:26:15.605581\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.686923980712891GiB\n",
      "Dsik free: 5.483486175537109GiB\n",
      "Time on CPU: 7:46:03.790000\n",
      "Model Accuracy:  0.7816\n",
      "Model f1-score:  0.7840113017879425\n",
      "Model weightedPrecision:  0.7998494840023584\n",
      "Model weightedRecall:  0.7816\n",
      "\n",
      "\n",
      " AFTER CALL TO FUNCTION\n",
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 3.7%\n",
      "Uptime: 19:26:42.111897\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.540996551513672GiB\n",
      "Dsik free: 5.484184265136719GiB\n",
      "Time on CPU: 7:47:05.590000\n",
      "Execution Time: 26.405650854110718\n",
      "CPU times: user 1.27 s, sys: 238 ms, total: 1.51 s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for pipeline, pipe_name in zip([pipeline_h1, pipeline_h2],[\"Pipeline 1\",\"Pipeline 2 with new stop words\"]):\n",
    "    print(\"#######\"+ pipe_name + \"#######\")\n",
    "    print(\"**\"+ \"Creating pipeline model\" + \"**\") \n",
    "    model_pipeline = get_pipeline_model(pipeline, training)\n",
    "    print(\"**\"+ \"Transforming training data\" + \"**\") \n",
    "    data_train  = model_pipeline.transform(training)\n",
    "    print(\"**\"+ \"Transforming testing data\" + \"**\") \n",
    "    data_test  = model_pipeline.transform(testing)\n",
    "    print(\"**\"+ \"Hyperparameter tuning\" + \"**\") \n",
    "    cv = get_best_smoothing_values(\"label\", \"prediction\")\n",
    "    print(\"**\"+ \"Training the model\" + \"**\") \n",
    "    cvModel = cv.fit(data_train)\n",
    "    print(\"**\"+ \"predicting on testing dataset\" + \"**\") \n",
    "    cvPredictions = cvModel.transform(data_test)\n",
    "    print(\"**\"+ \"Measuring performance\" + \"**\")\n",
    "    evaluate_model(\"label\", \"prediction\", cvPredictions.select(\"label\",\"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-stanley",
   "metadata": {},
   "source": [
    "# Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "rolled-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(fun):\n",
    "    \n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('BEFORE CALL TO FUNCTION')\n",
    "        system_info()\n",
    "        start = time()\n",
    "        rv = fun(*args, **kwargs)\n",
    "        duration = time() - start\n",
    "        print('\\n\\n AFTER CALL TO FUNCTION')\n",
    "        system_info()\n",
    "        print(\"Execution Time: {}\".format(duration))\n",
    "        return rv\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "transparent-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_info():\n",
    "    info = {\n",
    "        \"Uptime\": timedelta(seconds=time()-psu.boot_time()),\n",
    "        \"CPU in use\": \"{}%\".format(psu.cpu_percent(interval=.1)),\n",
    "        \"Time on CPU\": timedelta(seconds=psu.cpu_times().system+psu.cpu_times().user),\n",
    "        \"Memory in use\": \"{}GiB\".format(psu.virtual_memory().available/(1024**3)),\n",
    "        \"Disk in use\": \"{}%\".format(psu.disk_usage('/').percent),\n",
    "        \"Disk free\": \"{}GiB\".format(psu.disk_usage('/').free/(1024**3)),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    print(\"\\n\\n SYSTEMINFO\\n\\n\" + \"\\n\".join([\"{}: {}\".format(key,value) for key,value in info.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sunset-belle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " SYSTEMINFO\n",
      "\n",
      "CPU in use: 15.0%\n",
      "Uptime: 19:15:26.625819\n",
      "Disk in use: 65.5%\n",
      "Memory in use: 6.625919342041016MiB\n",
      "Dsik free: 5.499961853027344MiB\n",
      "Time on CPU: 7:32:57.350000\n"
     ]
    }
   ],
   "source": [
    "system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "wanted-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:15:19.796094\n"
     ]
    }
   ],
   "source": [
    "print(timedelta(seconds=time()-psu.boot_time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "crazy-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds=time()-psu.boot_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bridal-practice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-winning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
