{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "armed-deputy",
   "metadata": {},
   "source": [
    "# PySpark Implementation\n",
    "In this notebook, we will implement train a NB model using the training set & preprocess the data with our heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-condition",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "nominated-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, sys\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opened-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spark session.\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impossible-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-convenience",
   "metadata": {},
   "source": [
    "### Converting training data into RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incomplete-deviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "|      Id|               Title|                Body|                Tags|       CreationDate|       Y|\n",
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "|34552656|Java: Repeat Task...|<p>I'm already fa...|      <java><repeat>|2016-01-01 00:21:59|LQ_CLOSE|\n",
      "|34553034|Why are Java Opti...|<p>I'd like to un...|    <java><optional>|2016-01-01 02:03:20|      HQ|\n",
      "|34553174|Text Overlay Imag...|<p>I am attemptin...|<javascript><imag...|2016-01-01 02:48:24|      HQ|\n",
      "|34553318|Why ternary opera...|<p>The question i...|<swift><operators...|2016-01-01 03:30:17|      HQ|\n",
      "|34553755|hide/show fab wit...|<p>I'm using cust...|<android><materia...|2016-01-01 05:21:48|      HQ|\n",
      "+--------+--------------------+--------------------+--------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename_train = \"../dataset/train.csv\"\n",
    "filename_test = \"../dataset/valid.csv\"\n",
    "\n",
    "train_rdd = spark.read.csv(filename_train, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "test_rdd = spark.read.csv(filename_test, header=True, multiLine=True, inferSchema=True, escape='\"', quote='\"')\n",
    "train_rdd.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-elephant",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "In this part, we will preprocess our data with three different heuristics. The first using tokenization, second using stemming, third removing stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-anatomy",
   "metadata": {},
   "source": [
    "### Build training & testing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "careful-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = train_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"]) \\\n",
    "    .limit(5) # change to collect()\n",
    "\n",
    "testing = test_rdd.rdd \\\n",
    "    .map(lambda x: (x[\"Title\"]+\" \"+x[\"Body\"], x[\"Y\"])) \\\n",
    "    .toDF([\"Question\", \"Output\"]) \\\n",
    "    .limit(5) # change to collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-knife",
   "metadata": {},
   "source": [
    "### Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "structured-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEURISTIC 1 - Tokenize the words\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Question\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# HEURISTIC 2 - Remove the stopwords\n",
    "add_stopwords = [\"the\", \"a\", \"be\", \"of\", \"and\", \"to\", \"why\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# HEURISTIC 3 - Stem words\n",
    "# Use this library (TODO) - https://github.com/master/spark-stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aware-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build B.o.W model \n",
    "countVectors_h1 = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "indexed_features_h1 = StringIndexer(inputCol = \"Output\", outputCol = \"label\")\n",
    "\n",
    "countVectors_h2 = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "indexed_features_h2 = StringIndexer(inputCol = \"Output\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-chance",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "asian-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with Heurisitc 1\n",
    "pipeline_h1 = Pipeline(stages=[regexTokenizer, countVectors_h1, indexed_features_h1])\n",
    "\n",
    "# Pipeline with Heurisitc 2\n",
    "pipeline_h2 = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors_h2, indexed_features_h2])\n",
    "\n",
    "# Pipeline with Heurisitc 3\n",
    "# pipeline_h3 = Pipeline(stages=[regexTokenizer, stopwordsRemover, stemRemover, countVectors, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "optimum-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|Java: Repeat Task...|LQ_CLOSE|[java, repeat, ta...|(321,[0,5,6,7,8,1...|  1.0|\n",
      "|Why are Java Opti...|      HQ|[why, are, java, ...|(321,[0,5,7,10,26...|  0.0|\n",
      "|Text Overlay Imag...|      HQ|[text, overlay, i...|(321,[0,1,2,3,4,5...|  0.0|\n",
      "|Why ternary opera...|      HQ|[why, ternary, op...|(321,[0,1,5,6,7,8...|  0.0|\n",
      "|hide/show fab wit...|      HQ|[hide, show, fab,...|(321,[0,1,5,7,8,1...|  0.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|            Question|  Output|               words|            filtered|            features|label|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "|Java: Repeat Task...|LQ_CLOSE|[java, repeat, ta...|[java, repeat, ta...|(314,[0,5,7,16,22...|  1.0|\n",
      "|Why are Java Opti...|      HQ|[why, are, java, ...|[are, java, optio...|(314,[0,7,8,24,28...|  0.0|\n",
      "|Text Overlay Imag...|      HQ|[text, overlay, i...|[text, overlay, i...|(314,[0,1,2,3,4,5...|  0.0|\n",
      "|Why ternary opera...|      HQ|[why, ternary, op...|[ternary, operato...|(314,[0,1,5,7,8,1...|  0.0|\n",
      "|hide/show fab wit...|      HQ|[hide, show, fab,...|[hide, show, fab,...|(314,[0,1,7,8,22,...|  0.0|\n",
      "+--------------------+--------+--------------------+--------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_h1_fit = pipeline_h1.fit(training)\n",
    "data_h1 = pipeline_h1_fit.transform(training)\n",
    "data_h1.show()\n",
    "\n",
    "pipeline_h2_fit = pipeline_h2.fit(training)\n",
    "data_h2 = pipeline_h2_fit.transform(training)\n",
    "data_h2.show()\n",
    "\n",
    "# pipeline_h3_fit = pipeline_h3.fit(training)\n",
    "# data_h3 = pipeline_h3_fit.transform(training)\n",
    "# data_h3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-prospect",
   "metadata": {},
   "source": [
    "## Model training\n",
    "In this part, we will train various pipelines through a NB classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
